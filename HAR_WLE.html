<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Introduction</h2>

<p>Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community , especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.</p>

<p>The human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict &quot;which&quot; activity was performed at a specific point in time. This Weight Lifting Exercises dataset is to investigate &quot;how (well)&quot; an activity was performed by the wearer. The &quot;how (well)&quot; investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.</p>

<h2>Backgroud</h2>

<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways, the goal was to use data from accelerometers on the belt, forearm, arm, and dumbell of these 6 participants and predict the manner of their exercise</p>

<h2>1. Read the data</h2>

<pre><code class="r">wle = read.csv(&quot;pml-training.csv&quot;, stringsAsFactors = FALSE)
dim(wle)
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<h2>2. load library</h2>

<pre><code class="r">library(caret)
library(ggplot2)
library(foreach)
library(randomForest)
</code></pre>

<h2>3.Pre-Processing</h2>

<h3>3.1 Deal with Missing values</h3>

<p>Look at the proportion of NA or blank values of each feature</p>

<pre><code class="r">features = colnames(wle)
mv = rep(NA,ncol(wle))
for (i in 1:ncol(wle)) {
  nan = sum(is.na(wle[,i]) | wle[,i]==&#39;&#39; | wle[,i]==&#39; &#39;)
  mv[i] = nan/nrow(wle)
}
</code></pre>

<p>Drop the variables with missing / blank values more than 90%</p>

<pre><code class="r">names(mv) = features  # match the header with the missing value percentage
to_drop = names(mv[mv&gt;0.9]) # will remove the features with more than 90% missing value
ind = match(to_drop, features)  
wle_new = wle[,-ind]
dim(wle_new) 
</code></pre>

<pre><code>## [1] 19622    60
</code></pre>

<p>After removing the missing and blank values, There are 60 features left</p>

<h3>3.2 Feature Engineering</h3>

<h3>3.2.1 Categorical features</h3>

<p>Only two categorical variables: User_name and new_window</p>

<pre><code class="r">table(wle_new$user_name, wle_new$classe)
</code></pre>

<pre><code>##           
##               A    B    C    D    E
##   adelmo   1165  776  750  515  686
##   carlitos  834  690  493  486  609
##   charles   899  745  539  642  711
##   eurico    865  592  489  582  542
##   jeremy   1177  489  652  522  562
##   pedro     640  505  499  469  497
</code></pre>

<pre><code class="r">table(wle_new$new_window, wle_new$classe) 
</code></pre>

<pre><code>##      
##          A    B    C    D    E
##   no  5471 3718 3352 3147 3528
##   yes  109   79   70   69   79
</code></pre>

<p>From the tables above, there are no pattern between these two categorical variable and the response variable (classe)
Therefore we are going to drop User_name and new_window</p>

<h3>3.2.2 Time features</h3>

<p>Intuitively, there should not be any correlation between the date/time of the experiment conducted and the result of the experimebnt
But let&#39;s take a look at it</p>

<pre><code class="r">g11 &lt;- ggplot(wle_new, aes(raw_timestamp_part_1, color = classe)) + geom_density()
g11
</code></pre>

<p><img src="figure/unnamed-chunk-6-1.png" alt="plot of chunk unnamed-chunk-6"></p>

<pre><code class="r">g12 &lt;- ggplot(wle_new, aes(raw_timestamp_part_2, color = classe)) + geom_density()
g12
</code></pre>

<p><img src="figure/unnamed-chunk-6-2.png" alt="plot of chunk unnamed-chunk-6"></p>

<pre><code class="r">g13 &lt;- ggplot(wle_new, aes(cvtd_timestamp, color = classe)) + geom_density()
g13 
</code></pre>

<p><img src="figure/unnamed-chunk-6-3.png" alt="plot of chunk unnamed-chunk-6"></p>

<p>From the above density plots, we can clearly see that there is no connection between the 3 time variables and the outcome of the exercise, So we will drop them as well</p>

<h3>Remove the unnecessary features</h3>

<pre><code class="r">wle_new = wle_new[,7:60]
</code></pre>

<h3>3.2.3 Numerical features</h3>

<h3>correlation analasis</h3>

<p>Build a correlation matrix</p>

<pre><code class="r">corr.matrix = cor(wle_new[, 1:53], use = &quot;pairwise.complete.obs&quot;)
corr.matrix[is.na(corr.matrix)] = 0
</code></pre>

<p>Figure out which features are highly correlated ##</p>

<pre><code class="r">corr_features = foreach(i = 1:nrow(corr.matrix))  %do% {
  rownames(corr.matrix[corr.matrix[,i] &gt; 0.9,])
}

corr_features = corr_features[sapply(corr_features, function(x) length(x) &gt; 0 )] ## remove empty rows
corr_features = unique(corr_features) ## remove duplicates
corr_features
</code></pre>

<pre><code>## [[1]]
## [1] &quot;roll_belt&quot;        &quot;total_accel_belt&quot; &quot;accel_belt_y&quot;    
## 
## [[2]]
## [1] &quot;gyros_dumbbell_z&quot; &quot;gyros_forearm_z&quot;
</code></pre>

<p>There are 2 sets of correlated features and we will create new features to replace the correlated features</p>

<pre><code class="r">new_gyro = wle_new$gyros_dumbbell_z * wle_new$gyros_forearm_z
cor(wle_new$gyros_dumbbell_z, new_gyro)
</code></pre>

<pre><code>## [1] 0.9901482
</code></pre>

<pre><code class="r">cor(wle_new$gyros_forearm_z, new_gyro)
</code></pre>

<pre><code>## [1] 0.939303
</code></pre>

<pre><code class="r">new_belt = wle_new$total_accel_belt - wle_new$accel_belt_y - wle_new$roll_belt
cor(new_belt, wle_new$roll_belt)
</code></pre>

<pre><code>## [1] -0.9917479
</code></pre>

<pre><code class="r">cor(new_belt, wle_new$accel_belt_y)
</code></pre>

<pre><code>## [1] -0.9655505
</code></pre>

<p>Now we will replace the correlated features as indicated above with the new features</p>

<pre><code class="r">wle_new$new_gyro = new_gyro
wle_new$new_belt = new_belt
</code></pre>

<p>Remove the correlated features:</p>

<pre><code class="r">cor_f = c(corr_features[[1]], corr_features[[2]])
cor_ind = match(cor_f, colnames(wle_new))
## remove them
wle_new = wle_new[,-cor_ind]
</code></pre>

<p>So far we have total of 19622 observations and 51 features that we will use for modeling</p>

<pre><code class="r">dim(wle_new)
</code></pre>

<pre><code>## [1] 19622    51
</code></pre>

<p>check missing value one more time</p>

<pre><code class="r">sum(is.na(wle_new))
</code></pre>

<pre><code>## [1] 0
</code></pre>

<h3>3.2.4 Split the data to Train and CV sets</h3>

<pre><code class="r">training = wle_new
training$classe = as.factor(training$classe)
</code></pre>

<p>Will split the data into 80% Training and 20% validation sets</p>

<pre><code class="r">set.seed(12345)
inTrain = createDataPartition(y = training$classe, p=0.8, list = FALSE)
training_train = training[inTrain,]
training_cv = training[-inTrain,]
dim(training_train);dim(training_cv)
</code></pre>

<pre><code>## [1] 15699    51
</code></pre>

<pre><code>## [1] 3923   51
</code></pre>

<h2>4 Model Training</h2>

<h3>Random Forest Classifier for a Multi-Class Classification problem</h3>

<p>For this problem, I chose Random Forest because of its robustness to sparsity and multi-colinearity, as well as its generalization to the test data.</p>

<pre><code class="r">rf_fit = randomForest(classe ~., data = training_train, ntree=500, mtry=10)
pred_rf = predict(rf_fit, newdata = training_cv, type = &#39;response&#39;)
table(prediction = pred_rf, actual = training_cv$classe)
</code></pre>

<pre><code>##           actual
## prediction    A    B    C    D    E
##          A 1116    0    0    0    0
##          B    0  759    8    0    0
##          C    0    0  676    5    0
##          D    0    0    0  638    2
##          E    0    0    0    0  719
</code></pre>

<pre><code class="r">Pred_Acuracy = sum(diag(table(prediction = pred_rf, actual = training_cv$classe)))/length(training_cv$classe)
paste(&quot;Prediction Accuracy on the CV set is &quot;, Pred_Acuracy)
</code></pre>

<pre><code>## [1] &quot;Prediction Accuracy on the CV set is  0.9961763956156&quot;
</code></pre>

<h4>Since a 99.6% prediction accuracy is achieved , therefore I decided that there is no need to tune parameters in order to improve the model performance</h4>

<h2>5, Test the model with Testing data</h2>

<h3>5.1 apply the same Pre-Procesing method to test data</h3>

<pre><code class="r">test = read.csv(&quot;pml-testing.csv&quot;, stringsAsFactors = FALSE)
test_new = test[,-ind]
test_new = test_new[,7:60]
test_new$new_gyro = test_new$gyros_dumbbell_z * test_new$gyros_forearm_z
test_new$new_belt = test_new$total_accel_belt - test_new$accel_belt_y - test_new$roll_belt
test_new = test_new[,-cor_ind]
dim(test_new) ### make sure we have the same feasure space as training data
</code></pre>

<pre><code>## [1] 20 51
</code></pre>

<h3>5.2 Predict with trained Random Forest classifier</h3>

<pre><code class="r">test_pred = predict(rf_fit, newdata = test_new, type = &#39;response&#39;)
test_pred
</code></pre>

<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
</code></pre>

<h2>Reference</h2>

<p>Data Source
    - <a href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har</a></p>

</body>

</html>
